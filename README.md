Like last year, we'll start with version `v0` of some code, and we'll progressively improve the code as we iterate version numbers. Last year, we wrote our own _t_-test functions as a toy example, and so we were able to emphasize how to build abstractions, modularization, et cetera, but I also pointed out that it would rarely actually be a good idea to write your own function to do a _t_-test when you could just use one of the many implementations already out there... which are well maintained, rigorously tested, and used by thousands of people who will notice if there's a bug. We also covered how to write unit tests last year, which I don't (currently) get to in this repository, so check out [last year's](https://github.com/john-veillette/code-best-practices) repo for some quality content if you want to get more in depth with those things.

This year, we'll revise some code from an actual scientific project. That means that, rather than solely create our own abstractions/functions/etc. as if the rest of the scientific software ecosystem didn't exist, we'll moreso use abstractions from existing packages like you'd _actually_ want to do in your own projects. That'll be the big change from `v0` to `v1`, and you'll see how that cleans things up a whole lot. 

Going from `v1` to `v2`, we _will_ create our own abstractions, as we'll be offloading almost all of the code into a seperate Python module with a documented interface (i.e. an API, as the techies call it) that we can call from our main scripts. That way, we don't have to think about all the gnarly details in our main analysis code. (The "main analysis" isn't actually in this repository, but Google Scholar stalk me in a few months and hopefully you can see the end product by then.)
